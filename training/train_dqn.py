# train_dqn_fixed.py
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import sys
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ä¿®å¤åçš„æ¨¡å‹
from models.dqn_ai import DQNAgent
from models.rule_based_ai import RuleBasedAI  # ä½¿ç”¨ä¿®å¤åçš„è§„åˆ™AI


class TrainingGomokuEnv:
    """è®­ç»ƒç¯å¢ƒ"""

    def __init__(self, board_size=9):
        self.board_size = board_size
        self.reset()

    def reset(self):
        self.board = np.zeros((self.board_size, self.board_size), dtype=int)
        self.current_player = 1
        self.done = False
        self.winner = None
        return self.board.copy()

    def get_valid_moves(self):
        """è·å–æ‰€æœ‰åˆæ³•ç§»åŠ¨"""
        n = self.board_size
        valid_moves = np.zeros(n * n, dtype=int)
        for y in range(n):
            for x in range(n):
                if self.board[y][x] == 0:
                    action = y * n + x
                    valid_moves[action] = 1
        return valid_moves

    def is_valid_move(self, action):
        if action is None:
            return False
        n = self.board_size
        x, y = action % n, action // n
        return 0 <= x < n and 0 <= y < n and self.board[y][x] == 0

    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        if not self.is_valid_move(action) or self.done:
            return self.board.copy(), 0, True, {}

        n = self.board_size
        x, y = action % n, action // n
        self.board[y][x] = self.current_player

        # æ£€æŸ¥æ˜¯å¦è·èƒœ
        if self.check_win(y, x):
            self.done = True
            self.winner = self.current_player
            reward = 1.0 if self.current_player == 1 else -1.0
        elif np.all(self.board != 0):  # å¹³å±€
            self.done = True
            self.winner = 0
            reward = 0.1
        else:
            self.current_player = 3 - self.current_player
            reward = 0.0

        return self.board.copy(), reward, self.done, {}

    def check_win(self, y, x):
        """æ£€æŸ¥æ˜¯å¦è·èƒœ"""
        player = self.board[y][x]
        n = self.board_size

        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for dy, dx in directions:
            count = 1

            # æ­£å‘
            for i in range(1, 5):
                ny, nx = y + dy * i, x + dx * i
                if 0 <= ny < n and 0 <= nx < n and self.board[ny][nx] == player:
                    count += 1
                else:
                    break

            # åå‘
            for i in range(1, 5):
                ny, nx = y - dy * i, x - dx * i
                if 0 <= ny < n and 0 <= nx < n and self.board[ny][nx] == player:
                    count += 1
                else:
                    break

            if count >= 5:
                return True

        return False


def train_dqn(config):
    """è®­ç»ƒDQNæ¨¡å‹"""
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒDQNæ¨¡å‹")
    print("=" * 60)

    # åˆ›å»ºç¯å¢ƒ
    env = TrainingGomokuEnv(config['board_size'])

    # åˆ›å»ºæ™ºèƒ½ä½“
    print(f"åˆ›å»ºDQNæ™ºèƒ½ä½“ (æ£‹ç›˜å¤§å°: {config['board_size']}x{config['board_size']})")
    agent = DQNAgent(
        board_size=config['board_size'],
        player=1,
        lr=config['learning_rate'],
        gamma=config['gamma'],
        epsilon=config['epsilon'],
        epsilon_min=config['epsilon_min'],
        epsilon_decay=config['epsilon_decay'],
        target_update=config['target_update'],
        memory_size=config['memory_size']
    )

    # åˆ›å»ºå¯¹æ‰‹
    print("åˆ›å»ºè§„åˆ™AIå¯¹æ‰‹...")
    opponent = RuleBasedAI(
        player=2,
        board_size=config['board_size'],
        aggression=config['opponent_aggression'],
        debug=False
    )

    # è®­ç»ƒç»Ÿè®¡
    stats = {
        'episode': [],
        'total_reward': [],
        'avg_loss': [],
        'epsilon': [],
        'win_rate': [],
        'steps': [],
        'memory_size': []
    }

    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join(config['save_dir'], f"train_{timestamp}")
    os.makedirs(save_dir, exist_ok=True)

    print(f"ä¿å­˜ç›®å½•: {save_dir}")
    print(f"è®­ç»ƒè½®æ•°: {config['total_episodes']}")
    print(f"è¯„ä¼°é—´éš”: æ¯{config['eval_interval']}è½®")
    print(f"è¯„ä¼°å±€æ•°: {config['eval_games']}å±€")
    print("-" * 40)

    best_win_rate = 0
    patience_counter = 0
    max_patience = config.get('patience', 10)

    # è®­ç»ƒå¾ªç¯
    for episode in tqdm(range(config['total_episodes']), desc="è®­ç»ƒè¿›åº¦"):
        state = env.reset()
        done = False
        total_reward = 0
        total_loss = 0
        step_count = 0

        # éšæœºå†³å®šå…ˆæ‰‹
        agent_first = np.random.random() < 0.5
        agent.player = 1 if agent_first else 2
        opponent.player = 2 if agent_first else 1

        # ä¿å­˜å½“å‰ç©å®¶ç”¨äºç»éªŒå›æ”¾
        current_player_perspective = agent.player

        while not done:
            valid_moves = env.get_valid_moves()

            # å½“å‰ç©å®¶è¡ŒåŠ¨
            if env.current_player == agent.player:
                # DQN Agentè¡ŒåŠ¨
                action = agent.get_move(state, valid_moves, training=True)
                if action is None or not env.is_valid_move(action):
                    # æ²¡æœ‰åˆæ³•ç§»åŠ¨ï¼Œç»“æŸæ¸¸æˆ
                    done = True
                    if not done:  # å¦‚æœè¿˜æ²¡ç»“æŸï¼Œå¹³å±€
                        env.done = True
                        env.winner = 0
                    break

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)

                # è®¡ç®—æœ€ç»ˆå¥–åŠ±
                if done:
                    if env.winner == agent.player:
                        final_reward = 1.0
                    elif env.winner == opponent.player:
                        final_reward = -1.0
                    else:  # å¹³å±€
                        final_reward = 0.1
                else:
                    final_reward = 0.0

                # ä¿å­˜ç»éªŒ
                agent.remember(
                    state=state.copy(),
                    action=action,
                    reward=final_reward,
                    next_state=next_state.copy() if not done else None,
                    done=done,
                    valid_moves=valid_moves.copy(),
                    player=current_player_perspective
                )

                # è®­ç»ƒ
                if len(agent.memory) >= agent.batch_size:
                    loss = agent.replay()
                    if loss is not None:
                        total_loss += loss

                total_reward += final_reward
                state = next_state

            else:
                # å¯¹æ‰‹è¡ŒåŠ¨
                action = opponent.get_move(state, valid_moves)
                if action is None or not env.is_valid_move(action):
                    # æ²¡æœ‰åˆæ³•ç§»åŠ¨ï¼Œç»“æŸæ¸¸æˆ
                    done = True
                    if not done:  # å¦‚æœè¿˜æ²¡ç»“æŸï¼Œå¹³å±€
                        env.done = True
                        env.winner = 0
                    break

                state, reward, done, _ = env.step(action)

            step_count += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if step_count > config['board_size'] * config['board_size']:
                done = True
                env.done = True
                env.winner = 0  # å¹³å±€

        # è®°å½•ç»Ÿè®¡
        stats['episode'].append(episode)
        stats['total_reward'].append(total_reward)
        stats['avg_loss'].append(total_loss / max(step_count, 1))
        stats['epsilon'].append(agent.epsilon)
        stats['steps'].append(step_count)
        stats['memory_size'].append(len(agent.memory))

        # å®šæœŸè¯„ä¼°
        if (episode + 1) % config['eval_interval'] == 0 or episode == 0:
            # è¯„ä¼°
            win_rate = evaluate_agent(agent, opponent, config)

            # è®°å½•èƒœç‡
            stats['win_rate'].append(win_rate)

            # è·å–è®­ç»ƒä¿¡æ¯
            info = agent.get_training_info()

            # æ‰“å°è¿›åº¦
            print(f"\nè½®æ•° {episode + 1}/{config['total_episodes']}:")
            print(f"  èƒœç‡: {win_rate:.2%}")
            print(f"  å¹³å‡å¥–åŠ±: {total_reward:.3f}")
            print(f"  æ¢ç´¢ç‡: {agent.epsilon:.4f}")
            print(f"  è®°å¿†åº“å¤§å°: {len(agent.memory)}")
            print(f"  å¹³å‡æŸå¤±: {total_loss / max(step_count, 1):.6f}")
            print(f"  å¹³å‡æ­¥æ•°: {step_count}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if win_rate > best_win_rate:
                best_win_rate = win_rate
                patience_counter = 0

                # ä¿å­˜æ¨¡å‹
                model_path = os.path.join(save_dir, f"best_model_ep{episode + 1}_wr{win_rate:.3f}.pth")
                agent.save(model_path)
                print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}")

                # æ—©åœæ£€æŸ¥
                if win_rate >= config.get('target_win_rate', 0.7):
                    print(f"\nğŸ‰ è¾¾åˆ°ç›®æ ‡èƒœç‡ {win_rate:.2%}ï¼Œæå‰åœæ­¢è®­ç»ƒï¼")
                    break
            else:
                patience_counter += 1

            # æ—©åœï¼šé•¿æ—¶é—´æ²¡æœ‰æå‡
            if patience_counter >= max_patience:
                print(f"\nâš ï¸  {max_patience}æ¬¡è¯„ä¼°æ²¡æœ‰æå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(save_dir, "final_model.pth")
    agent.save(final_path)
    print(f"\nâœ… ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(stats, save_dir, config)

    return agent, stats, save_dir


def evaluate_agent(agent, opponent, config, verbose=False):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    env = TrainingGomokuEnv(config['board_size'])
    wins = 0
    total_games = config['eval_games']

    for game_idx in range(total_games):
        state = env.reset()
        done = False

        # éšæœºå†³å®šå…ˆæ‰‹
        agent_first = np.random.random() < 0.5
        agent.player = 1 if agent_first else 2
        opponent.player = 2 if agent_first else 1

        while not done:
            valid_moves = env.get_valid_moves()

            if env.current_player == agent.player:
                # DQN Agentè¡ŒåŠ¨
                action = agent.get_move(state, valid_moves, training=False)
            else:
                # å¯¹æ‰‹è¡ŒåŠ¨
                action = opponent.get_move(state, valid_moves)

            if action is None or not env.is_valid_move(action):
                # æ²¡æœ‰åˆæ³•ç§»åŠ¨
                env.done = True
                env.winner = 0
                done = True
                break

            state, reward, done, _ = env.step(action)

            if env.done:
                if env.winner == agent.player:
                    wins += 1
                break

        if verbose and (game_idx + 1) % 5 == 0:
            print(f"  è¯„ä¼°è¿›åº¦: {game_idx + 1}/{total_games}")

    win_rate = wins / total_games
    return win_rate


def plot_training_curves(stats, save_dir, config):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.figure(figsize=(15, 10))

    # 1. å¥–åŠ±æ›²çº¿
    plt.subplot(2, 3, 1)
    window = 50
    if len(stats['total_reward']) > window:
        rewards_smooth = np.convolve(stats['total_reward'], np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(stats['total_reward'])), rewards_smooth, 'b-', alpha=0.7, label='å¹³æ»‘')
    plt.plot(stats['total_reward'], 'b-', alpha=0.3, label='åŸå§‹')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('æ€»å¥–åŠ±')
    plt.title('å¥–åŠ±æ›²çº¿')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # 2. æŸå¤±æ›²çº¿
    plt.subplot(2, 3, 2)
    if len(stats['avg_loss']) > window:
        loss_smooth = np.convolve(stats['avg_loss'], np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(stats['avg_loss'])), loss_smooth, 'r-', alpha=0.7, label='å¹³æ»‘')
    plt.plot(stats['avg_loss'], 'r-', alpha=0.3, label='åŸå§‹')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('å¹³å‡æŸå¤±')
    plt.title('æŸå¤±æ›²çº¿')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # 3. æ¢ç´¢ç‡æ›²çº¿
    plt.subplot(2, 3, 3)
    plt.plot(stats['epsilon'], 'g-')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('æ¢ç´¢ç‡ (Îµ)')
    plt.title('æ¢ç´¢ç‡è¡°å‡')
    plt.grid(True, alpha=0.3)

    # 4. èƒœç‡æ›²çº¿
    plt.subplot(2, 3, 4)
    if stats['win_rate']:
        eval_points = [config['eval_interval'] * (i + 1) for i in range(len(stats['win_rate']))]
        plt.plot(eval_points, stats['win_rate'], 'mo-', linewidth=2, markersize=5)
        plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50%åŸºå‡†')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('èƒœç‡')
        plt.title('èƒœç‡ vs è§„åˆ™AI')
        plt.ylim(0, 1.05)
        plt.grid(True, alpha=0.3)
        plt.legend()

    # 5. æ­¥æ•°æ›²çº¿
    plt.subplot(2, 3, 5)
    if len(stats['steps']) > window:
        steps_smooth = np.convolve(stats['steps'], np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(stats['steps'])), steps_smooth, 'c-', alpha=0.7, label='å¹³æ»‘')
    plt.plot(stats['steps'], 'c-', alpha=0.3, label='åŸå§‹')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('æ­¥æ•°')
    plt.title('æ¯å±€å¹³å‡æ­¥æ•°')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # 6. è®°å¿†åº“å¤§å°
    plt.subplot(2, 3, 6)
    plt.plot(stats['memory_size'], 'y-')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('è®°å¿†æ•°é‡')
    plt.title('ç»éªŒå›æ”¾è®°å¿†åº“å¤§å°')
    plt.grid(True, alpha=0.3)

    plt.suptitle(f"DQNè®­ç»ƒç»“æœ - {config['board_size']}x{config['board_size']} æ£‹ç›˜", fontsize=16)
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    plot_path = os.path.join(save_dir, "training_curves.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®­ç»ƒé…ç½®
    config = {
        'board_size': 9,  # è®­ç»ƒ9x9æ£‹ç›˜
        'total_episodes': 2000,  # æ€»è®­ç»ƒè½®æ•°
        'learning_rate': 0.001,
        'gamma': 0.95,  # æŠ˜æ‰£å› å­
        'epsilon': 1.0,  # åˆå§‹æ¢ç´¢ç‡
        'epsilon_min': 0.1,  # æœ€å°æ¢ç´¢ç‡
        'epsilon_decay': 0.998,  # æ¢ç´¢ç‡è¡°å‡
        'target_update': 200,  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        'memory_size': 20000,  # è®°å¿†åº“å¤§å°
        'eval_interval': 50,  # è¯„ä¼°é—´éš”
        'eval_games': 20,  # æ¯æ¬¡è¯„ä¼°çš„å±€æ•°
        'save_dir': 'saved_models',  # ä¿å­˜ç›®å½•
        'opponent_aggression': 0.3,  # å¯¹æ‰‹æ”»å‡»æ€§ï¼ˆ0-1ï¼‰
        'target_win_rate': 0.7,  # ç›®æ ‡èƒœç‡
        'patience': 10  # æ—©åœè€å¿ƒå€¼
    }

    print("è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")

    # å¼€å§‹è®­ç»ƒ
    try:
        agent, stats, save_dir = train_dqn(config)

        # æœ€ç»ˆè¯„ä¼°
        print("\n" + "=" * 60)
        print("æœ€ç»ˆè¯„ä¼°")
        print("=" * 60)

        # åˆ›å»ºå¯¹æ‰‹
        opponent = RuleBasedAI(
            player=2,
            board_size=config['board_size'],
            aggression=0.3,
            debug=False
        )

        # è¯„ä¼°100å±€
        final_win_rate = evaluate_agent(agent, opponent, config, verbose=True)
        print(f"æœ€ç»ˆèƒœç‡: {final_win_rate:.2%}")

        # ä¿å­˜æœ€ç»ˆç»“æœ
        result_path = os.path.join(save_dir, "training_results.txt")
        with open(result_path, 'w') as f:
            f.write("DQNè®­ç»ƒç»“æœæ€»ç»“\n")
            f.write("=" * 40 + "\n")
            f.write(f"è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æœ€ç»ˆè½®æ•°: {len(stats['episode'])}\n")
            f.write(f"æœ€ç»ˆèƒœç‡: {final_win_rate:.2%}\n")
            f.write(f"æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.4f}\n")
            f.write(f"è®°å¿†åº“å¤§å°: {len(agent.memory)}\n")
            f.write(f"æ¨¡å‹ä¿å­˜ç›®å½•: {save_dir}\n")

        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åˆ°: {save_dir}")

    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()