# train_dqn_fixed.py
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import sys
import random
from datetime import datetime
import argparse
import json
from collections import deque

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.dqn_ai import DQNAgent
from models.rule_based_ai import RuleBasedAI
from models.random_ai import RandomAI


class TrainingEnvironment:
    """è®­ç»ƒç¯å¢ƒ"""

    def __init__(self, board_size=9):
        self.board_size = board_size
        self.reset()

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.board = np.zeros((self.board_size, self.board_size), dtype=int)
        self.current_player = 1
        self.done = False
        self.winner = None
        self.step_count = 0
        return self.board.copy()

    def get_valid_moves(self):
        """è·å–åˆæ³•ç§»åŠ¨"""
        n = self.board_size
        valid_moves = np.zeros(n * n, dtype=int)
        for y in range(n):
            for x in range(n):
                if self.board[y][x] == 0:
                    action = y * n + x
                    valid_moves[action] = 1
        return valid_moves

    def is_valid_move(self, action):
        """æ£€æŸ¥ç§»åŠ¨æ˜¯å¦åˆæ³•"""
        if action is None:
            return False
        n = self.board_size
        x, y = action % n, action // n
        return 0 <= x < n and 0 <= y < n and self.board[y][x] == 0

    def step(self, action, player):
        """æ‰§è¡Œä¸€æ­¥"""
        if not self.is_valid_move(action) or self.done:
            return self.board.copy(), 0, True, {}

        n = self.board_size
        x, y = action % n, action // n
        self.board[y][x] = player
        self.step_count += 1

        # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
        if self.check_win(x, y, player):
            self.done = True
            self.winner = player
            reward = 1.0
        elif np.all(self.board != 0):  # å¹³å±€
            self.done = True
            self.winner = 0
            reward = 0.1
        else:
            self.current_player = 3 - self.current_player
            reward = 0.0

        return self.board.copy(), reward, self.done, {}

    def check_win(self, x, y, player):
        """æ£€æŸ¥æ˜¯å¦è·èƒœ"""
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]

        for dx, dy in directions:
            count = 1

            # æ­£å‘
            for i in range(1, 5):
                nx, ny = x + dx * i, y + dy * i
                if 0 <= nx < self.board_size and 0 <= ny < self.board_size and self.board[ny][nx] == player:
                    count += 1
                else:
                    break

            # åå‘
            for i in range(1, 5):
                nx, ny = x - dx * i, y - dy * i
                if 0 <= nx < self.board_size and 0 <= ny < self.board_size and self.board[ny][nx] == player:
                    count += 1
                else:
                    break

            if count >= 5:
                return True

        return False


class DQNTrainer:
    """DQNè®­ç»ƒå™¨"""

    def __init__(self, config=None):
        self.config = config or self.get_default_config()
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®é¡¹éƒ½å­˜åœ¨
        self.config = {**self.get_default_config(), **(config or {})}
        self.setup_training()

    def get_default_config(self):
        """é»˜è®¤é…ç½®"""
        return {
            'opponent_type': 'mixed',  # 'random', 'rule', 'mixed', 'self', 'previous'
            'rule_aggression': 0.3,
            'mixed_ratio': 0.5,
            'board_size': 9,
            'total_episodes': 2000,
            'learning_rate': 0.001,
            'gamma': 0.99,
            'epsilon': 1.0,
            'epsilon_min': 0.01,
            'epsilon_decay': 0.998,
            'target_update': 100,
            'memory_size': 20000,
            'batch_size': 64,
            'eval_interval': 50,
            'eval_games': 20,
            'save_interval': 100,
            'save_dir': 'saved_models',
            'log_dir': 'training_logs',
            'early_stop_patience': 10,
            'target_win_rate': 0.7,
            'previous_model_path': None,  # æ—§ç‰ˆæœ¬æ¨¡å‹è·¯å¾„
            'self_play_ratio': 0.3,  # è‡ªæˆ‘å¯¹å¼ˆæ¯”ä¾‹
            'model_pool_size': 3,  # æ¨¡å‹æ± å¤§å°
            'pool_update_interval': 200  # æ¨¡å‹æ± æ›´æ–°é—´éš”
        }

    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        # åˆ›å»ºç›®å½•
        os.makedirs(self.config['save_dir'], exist_ok=True)
        os.makedirs(self.config['log_dir'], exist_ok=True)

        # åˆ›å»ºç¯å¢ƒ
        self.env = TrainingEnvironment(self.config['board_size'])

        # åˆ›å»ºDQNæ™ºèƒ½ä½“
        self.dqn_agent = DQNAgent(
            board_size=self.config['board_size'],
            player=1,
            lr=self.config['learning_rate'],
            gamma=self.config['gamma'],
            epsilon=self.config['epsilon'],
            epsilon_min=self.config['epsilon_min'],
            epsilon_decay=self.config['epsilon_decay'],
            target_update=self.config['target_update'],
            memory_size=self.config['memory_size']
        )

        # åŠ è½½æ—§ç‰ˆæœ¬æ¨¡å‹ï¼ˆå¦‚æœæä¾›ï¼‰
        if self.config['previous_model_path'] and os.path.exists(self.config['previous_model_path']):
            try:
                self.dqn_agent.load(self.config['previous_model_path'])
                self._log(f"âœ… æˆåŠŸåŠ è½½æ—§ç‰ˆæœ¬æ¨¡å‹: {self.config['previous_model_path']}")
            except Exception as e:
                self._log(f"âš ï¸ åŠ è½½æ—§ç‰ˆæœ¬æ¨¡å‹å¤±è´¥: {e}")

        # æ¨¡å‹æ± ï¼ˆç”¨äºè‡ªæˆ‘å¯¹å¼ˆï¼‰
        self.model_pool = []
        self.model_pool_update_counter = 0

        # è®­ç»ƒç»Ÿè®¡
        self.stats = {
            'episodes': [],
            'rewards': [],
            'losses': [],
            'epsilon': [],
            'win_rates': [],
            'steps': [],
            'memory_size': [],
            'opponent_types': []
        }

        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = os.path.join(self.config['log_dir'], f'training_{timestamp}.log')
        self._log(f"è®­ç»ƒé…ç½®: {json.dumps(self.config, indent=2)}")

    def get_opponent(self, episode):
        """æ ¹æ®é…ç½®è·å–å¯¹æ‰‹"""
        opponent_type = self.config['opponent_type']

        if opponent_type == 'random':
            return RandomAI(player=2), 'random'

        elif opponent_type == 'rule':
            # åŠ¨æ€è°ƒæ•´è§„åˆ™AIéš¾åº¦
            aggression = self.config['rule_aggression']
            if episode > self.config['total_episodes'] * 0.7:
                aggression = min(0.7, aggression + 0.2)  # åæœŸå¢åŠ éš¾åº¦
            return RuleBasedAI(player=2, board_size=self.config['board_size'],
                               aggression=aggression, debug=False), 'rule'

        elif opponent_type == 'mixed':
            # æ··åˆè®­ç»ƒ
            if episode < self.config['total_episodes'] * 0.3:
                # å‰æœŸä¸»è¦ç”¨éšæœºAI
                ratio = 0.2
            elif episode < self.config['total_episodes'] * 0.7:
                # ä¸­æœŸæ··åˆ
                ratio = self.config['mixed_ratio']
            else:
                # åæœŸä¸»è¦ç”¨è§„åˆ™AI
                ratio = 0.8

            if random.random() < ratio:
                aggression = random.uniform(0.2, 0.6)
                return RuleBasedAI(player=2, board_size=self.config['board_size'],
                                   aggression=aggression, debug=False), 'rule'
            else:
                return RandomAI(player=2), 'random'

        elif opponent_type == 'self':
            # è‡ªæˆ‘å¯¹å¼ˆ
            return self.create_self_play_opponent(episode), 'self'

        elif opponent_type == 'previous':
            # ä¸æ—§ç‰ˆæœ¬å¯¹å¼ˆ
            return self.create_previous_version_opponent(), 'previous'

        else:
            raise ValueError(f"æœªçŸ¥çš„å¯¹æ‰‹ç±»å‹: {opponent_type}")

    def create_self_play_opponent(self, episode):
        """åˆ›å»ºè‡ªæˆ‘å¯¹å¼ˆå¯¹æ‰‹"""
        # ä»æ¨¡å‹æ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨å½“å‰æ¨¡å‹
        if self.model_pool and random.random() < 0.7:
            # 70%çš„æ¦‚ç‡ä»æ¨¡å‹æ± ä¸­é€‰æ‹©
            opponent_model = random.choice(self.model_pool)
            opponent = DQNAgent(
                board_size=self.config['board_size'],
                player=2,
                lr=self.config['learning_rate'],
                gamma=self.config['gamma'],
                epsilon=0.01,  # è¯„ä¼°æ¨¡å¼ï¼Œæ¢ç´¢ç‡ä½
                epsilon_min=0.01,
                epsilon_decay=1.0
            )
            # å¤åˆ¶æ¨¡å‹æƒé‡
            opponent.policy_net.load_state_dict(opponent_model['state_dict'])
            opponent.target_net.load_state_dict(opponent_model['state_dict'])
            return opponent
        else:
            # 30%çš„æ¦‚ç‡ä½¿ç”¨å½“å‰æ¨¡å‹
            opponent = DQNAgent(
                board_size=self.config['board_size'],
                player=2,
                lr=self.config['learning_rate'],
                gamma=self.config['gamma'],
                epsilon=0.05,  # ç¨å¾®æœ‰ç‚¹æ¢ç´¢
                epsilon_min=0.01,
                epsilon_decay=1.0
            )
            opponent.policy_net.load_state_dict(self.dqn_agent.policy_net.state_dict())
            opponent.target_net.load_state_dict(self.dqn_agent.target_net.state_dict())
            return opponent

    def create_previous_version_opponent(self):
        """åˆ›å»ºæ—§ç‰ˆæœ¬å¯¹æ‰‹"""
        if self.config['previous_model_path'] and os.path.exists(self.config['previous_model_path']):
            opponent = DQNAgent(
                board_size=self.config['board_size'],
                player=2,
                lr=self.config['learning_rate'],
                gamma=self.config['gamma'],
                epsilon=0.02,
                epsilon_min=0.01,
                epsilon_decay=1.0
            )
            opponent.load(self.config['previous_model_path'])
            return opponent
        else:
            # å¦‚æœæ²¡æœ‰æ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨è§„åˆ™AI
            self._log("âš ï¸ æœªæ‰¾åˆ°æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ™AIä»£æ›¿")
            return RuleBasedAI(player=2, board_size=self.config['board_size'],
                               aggression=0.5, debug=False)

    def update_model_pool(self, episode):
        """æ›´æ–°æ¨¡å‹æ± """
        self.model_pool_update_counter += 1

        if self.model_pool_update_counter >= self.config['pool_update_interval']:
            self.model_pool_update_counter = 0

            # ä¿å­˜å½“å‰æ¨¡å‹åˆ°æ± ä¸­
            model_snapshot = {
                'episode': episode,
                'state_dict': self.dqn_agent.policy_net.state_dict().copy(),
                'timestamp': datetime.now()
            }

            self.model_pool.append(model_snapshot)

            # ä¿æŒæ¨¡å‹æ± å¤§å°
            if len(self.model_pool) > self.config['model_pool_size']:
                # ç§»é™¤æœ€æ—§çš„æ¨¡å‹
                self.model_pool.pop(0)

            self._log(f"ğŸ“¦ æ¨¡å‹æ± æ›´æ–°: å½“å‰å¤§å° {len(self.model_pool)}")

    def play_episode(self, opponent, opponent_type, episode):
        """è¿›è¡Œä¸€å±€æ¸¸æˆ"""
        state = self.env.reset()
        done = False
        total_reward = 0
        total_loss = 0
        step_count = 0

        # éšæœºå†³å®šå…ˆæ‰‹
        dqn_first = random.random() < 0.5
        dqn_player = 1 if dqn_first else 2
        opponent_player = 2 if dqn_first else 1

        self.dqn_agent.player = dqn_player
        opponent.player = opponent_player

        # ç”¨äºç»éªŒå›æ”¾çš„ç©å®¶è§†è§’
        current_perspective = dqn_player

        while not done and step_count < self.config['board_size'] ** 2:
            valid_moves = self.env.get_valid_moves()

            if self.env.current_player == dqn_player:
                # DQN AIçš„å›åˆ
                action = self.dqn_agent.get_move(state, valid_moves, training=True)

                if action is None or not self.env.is_valid_move(action):
                    break

                next_state, reward, done, _ = self.env.step(action, dqn_player)

                # è®¡ç®—æœ€ç»ˆå¥–åŠ±
                if done:
                    if self.env.winner == dqn_player:
                        final_reward = 1.0
                    elif self.env.winner == opponent_player:
                        final_reward = -1.0
                    else:
                        final_reward = 0.1
                else:
                    final_reward = 0.0

                # ä¿å­˜ç»éªŒ
                self.dqn_agent.remember(
                    state=state.copy(),
                    action=action,
                    reward=final_reward,
                    next_state=next_state.copy() if not done else None,
                    done=done,
                    valid_moves=valid_moves.copy(),
                    player=current_perspective
                )

                # è®­ç»ƒ
                if len(self.dqn_agent.memory) >= self.dqn_agent.batch_size:
                    loss = self.dqn_agent.replay()
                    if loss is not None:
                        total_loss += loss

                total_reward += final_reward
                state = next_state

            else:
                # å¯¹æ‰‹çš„å›åˆ
                action = opponent.get_move(state, valid_moves)
                if action is None or not self.env.is_valid_move(action):
                    break

                state, _, done, _ = self.env.step(action, opponent_player)

            step_count += 1

        # æ›´æ–°æ¨¡å‹æ± ï¼ˆå¦‚æœæ˜¯è‡ªæˆ‘å¯¹å¼ˆï¼‰
        if opponent_type == 'self':
            self.update_model_pool(episode)

        return total_reward, total_loss, step_count, self.env.winner == dqn_player

    def evaluate_agent(self, num_games=20):
        """ä¿®å¤ç‰ˆè¯„ä¼°å‡½æ•°"""
        wins = 0

        self._log(f"   å¼€å§‹è¯„ä¼°{num_games}å±€æ¸¸æˆ...")

        for game_idx in range(num_games):
            env = TrainingEnvironment(self.config['board_size'])
            state = env.reset()
            done = False

            # éšæœºå†³å®šå…ˆæ‰‹
            dqn_first = random.random() < 0.5
            dqn_player = 1 if dqn_first else 2
            opponent_player = 2 if dqn_first else 1

            # ä½¿ç”¨è§„åˆ™AIè¿›è¡Œè¯„ä¼°
            opponent = RuleBasedAI(player=opponent_player,
                                   board_size=self.config['board_size'],
                                   aggression=0.5, debug=False)

            self.dqn_agent.player = dqn_player

            step_count = 0
            while not done and step_count < 100:
                valid_moves = env.get_valid_moves()

                if env.current_player == dqn_player:
                    # DQN AIè¡ŒåŠ¨ï¼ˆå…³é—­è®­ç»ƒæ¨¡å¼ï¼‰
                    action = self.dqn_agent.get_move(state, valid_moves, training=False)
                else:
                    # å¯¹æ‰‹è¡ŒåŠ¨
                    action = opponent.get_move(state, valid_moves)

                if action is None or not env.is_valid_move(action):
                    break

                state, _, done, _ = env.step(action, env.current_player)
                step_count += 1

                if done:
                    # æ­£ç¡®åˆ¤æ–­èƒœè´Ÿ
                    if env.winner == dqn_player:
                        wins += 1
                        if game_idx < 3:  # åªæ˜¾ç¤ºå‰3å±€ç»“æœ
                            self._log(f"    ç¬¬{game_idx + 1}å±€: âœ… DQNè·èƒœ (æ­¥æ•°: {step_count})")
                    elif env.winner == opponent_player:
                        if game_idx < 3:
                            self._log(f"    ç¬¬{game_idx + 1}å±€: âŒ DQNå¤±è´¥ (æ­¥æ•°: {step_count})")
                    else:
                        if game_idx < 3:
                            self._log(f"    ç¬¬{game_idx + 1}å±€: ğŸ¤ å¹³å±€ (æ­¥æ•°: {step_count})")
                    break

        win_rate = wins / num_games
        self._log(f"  ğŸ“Š æœ€ç»ˆè¯„ä¼°: {wins}èƒœ/{num_games}å±€ = {win_rate:.2%}")
        return win_rate

    def _log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        print(log_message)

        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self._log("å¼€å§‹è®­ç»ƒDQN AI")
        self._log(f"å¯¹æ‰‹ç±»å‹: {self.config['opponent_type']}")
        self._log(f"è®­ç»ƒè½®æ•°: {self.config['total_episodes']}")

        best_win_rate = 0
        patience_counter = 0
        total_wins = 0

        for episode in tqdm(range(self.config['total_episodes']), desc="è®­ç»ƒè¿›åº¦"):
            # è·å–å¯¹æ‰‹
            opponent, opponent_type = self.get_opponent(episode)

            # è¿›è¡Œä¸€å±€æ¸¸æˆ
            total_reward, total_loss, step_count, won = self.play_episode(opponent, opponent_type, episode)

            if won:
                total_wins += 1

            # è®°å½•ç»Ÿè®¡
            self.stats['episodes'].append(episode)
            self.stats['rewards'].append(total_reward)
            self.stats['losses'].append(total_loss / max(step_count, 1))
            self.stats['epsilon'].append(self.dqn_agent.epsilon)
            self.stats['steps'].append(step_count)
            self.stats['memory_size'].append(len(self.dqn_agent.memory))
            self.stats['opponent_types'].append(opponent_type)

            # å®šæœŸè¯„ä¼°
            if (episode + 1) % self.config['eval_interval'] == 0 or episode == 0:
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                self._log(f"ğŸ” å¼€å§‹è¯„ä¼°ç¬¬{episode + 1}è½®...")
                # å…ˆæ˜¾ç¤ºç´¯è®¡èƒœç‡
                cumulative_win_rate = total_wins / (episode + 1) if episode > 0 else 0
                self._log(f"  ç´¯è®¡èƒœç‡: {cumulative_win_rate:.2%} ({total_wins}/{episode + 1})")

                win_rate = self.evaluate_agent(self.config['eval_games'])
                self.stats['win_rates'].append(win_rate)

                # è·å–è®­ç»ƒä¿¡æ¯
                info = self.dqn_agent.get_training_info()

                self._log(f"\nè½®æ•° {episode + 1}:")
                self._log(f"  è¯„ä¼°èƒœç‡: {win_rate:.2%} (ç´¯è®¡: {total_wins}/{episode + 1} = {cumulative_win_rate:.2%})")
                self._log(f"  æ€»å¥–åŠ±: {total_reward:.3f}")
                self._log(f"  æ¢ç´¢ç‡: {self.dqn_agent.epsilon:.4f}")
                self._log(f"  è®°å¿†åº“: {len(self.dqn_agent.memory)}")
                self._log(f"  å¹³å‡æŸå¤±: {total_loss / max(step_count, 1):.6f}")
                self._log(f"  æ­¥æ•°: {step_count}")
                self._log(f"  å¯¹æ‰‹ç±»å‹: {opponent_type}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if win_rate > best_win_rate:
                    best_win_rate = win_rate
                    patience_counter = 0

                    model_path = os.path.join(
                        self.config['save_dir'],
                        f"best_model_ep{episode + 1}_wr{win_rate:.3f}.pth"
                    )
                    self.dqn_agent.save(model_path)
                    self._log(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}")

                    # æ—©åœæ£€æŸ¥
                    if win_rate >= self.config['target_win_rate']:
                        self._log(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡èƒœç‡ {win_rate:.2%}ï¼Œæå‰åœæ­¢è®­ç»ƒï¼")
                        break
                else:
                    patience_counter += 1

                # æ—©åœï¼šé•¿æ—¶é—´æ²¡æœ‰æå‡
                if patience_counter >= self.config['early_stop_patience']:
                    self._log(f"â¹ï¸  {self.config['early_stop_patience']}æ¬¡è¯„ä¼°æ²¡æœ‰æå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                    break

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (episode + 1) % self.config['save_interval'] == 0:
                checkpoint_path = os.path.join(
                    self.config['save_dir'],
                    f"checkpoint_ep{episode + 1}.pth"
                )
                self.dqn_agent.save(checkpoint_path)
                self._log(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(self.config['save_dir'], "final_model.pth")
        self.dqn_agent.save(final_path)
        self._log(f"âœ… ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()

        return self.dqn_agent, self.stats

    def play_episode(self, opponent, opponent_type, episode):
        """è¿›è¡Œä¸€å±€æ¸¸æˆï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        state = self.env.reset()
        done = False
        total_reward = 0
        total_loss = 0
        step_count = 0

        # éšæœºå†³å®šå…ˆæ‰‹
        dqn_first = random.random() < 0.5
        dqn_player = 1 if dqn_first else 2
        opponent_player = 2 if dqn_first else 1

        self.dqn_agent.player = dqn_player
        opponent.player = opponent_player

        while not done and step_count < self.config['board_size'] ** 2:
            valid_moves = self.env.get_valid_moves()

            if self.env.current_player == dqn_player:
                # DQN AIçš„å›åˆ
                action = self.dqn_agent.get_move(state, valid_moves, training=True)

                if action is None or not self.env.is_valid_move(action):
                    break

                next_state, reward, done, _ = self.env.step(action, dqn_player)

                # è®¡ç®—æœ€ç»ˆå¥–åŠ±
                if done:
                    if self.env.winner == dqn_player:
                        final_reward = 1.0
                    elif self.env.winner == opponent_player:
                        final_reward = -1.0
                    else:
                        final_reward = 0.1
                else:
                    final_reward = 0.0

                # ä¿å­˜ç»éªŒ
                self.dqn_agent.remember(
                    state=state.copy(),
                    action=action,
                    reward=final_reward,
                    next_state=next_state.copy() if not done else None,
                    done=done,
                    valid_moves=valid_moves.copy(),
                    player=dqn_player
                )

                # è®­ç»ƒ
                if len(self.dqn_agent.memory) >= self.dqn_agent.batch_size:
                    loss = self.dqn_agent.replay()
                    if loss is not None:
                        total_loss += loss

                total_reward += final_reward
                state = next_state

            else:
                # å¯¹æ‰‹çš„å›åˆ
                action = opponent.get_move(state, valid_moves)
                if action is None or not self.env.is_valid_move(action):
                    break

                state, _, done, _ = self.env.step(action, opponent_player)

            step_count += 1

        return total_reward, total_loss, step_count, self.env.winner == dqn_player

    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(15, 12))

        # 1. èƒœç‡æ›²çº¿
        plt.subplot(3, 3, 1)
        if self.stats['win_rates']:
            eval_points = [self.config['eval_interval'] * (i + 1) for i in range(len(self.stats['win_rates']))]
            plt.plot(eval_points, self.stats['win_rates'], 'mo-', linewidth=2, markersize=5)
            plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50%åŸºå‡†')
            plt.axhline(y=self.config['target_win_rate'], color='g', linestyle='--', alpha=0.5, label='ç›®æ ‡èƒœç‡')
            plt.xlabel('è®­ç»ƒè½®æ•°')
            plt.ylabel('èƒœç‡')
            plt.title('èƒœç‡ vs è§„åˆ™AI')
            plt.ylim(0, 1.05)
            plt.grid(True, alpha=0.3)
            plt.legend()

        # 2. å¥–åŠ±æ›²çº¿
        plt.subplot(3, 3, 2)
        window = 50
        if len(self.stats['rewards']) > window:
            rewards_smooth = np.convolve(self.stats['rewards'], np.ones(window) / window, mode='valid')
            plt.plot(range(window - 1, len(self.stats['rewards'])), rewards_smooth, 'b-', alpha=0.7, label='å¹³æ»‘')
        plt.plot(self.stats['rewards'], 'b-', alpha=0.3, label='åŸå§‹')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('æ€»å¥–åŠ±')
        plt.title('å¥–åŠ±æ›²çº¿')
        plt.grid(True, alpha=0.3)
        plt.legend()

        # 3. æŸå¤±æ›²çº¿
        plt.subplot(3, 3, 3)
        if len(self.stats['losses']) > window:
            loss_smooth = np.convolve(self.stats['losses'], np.ones(window) / window, mode='valid')
            plt.plot(range(window - 1, len(self.stats['losses'])), loss_smooth, 'r-', alpha=0.7, label='å¹³æ»‘')
        plt.plot(self.stats['losses'], 'r-', alpha=0.3, label='åŸå§‹')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('å¹³å‡æŸå¤±')
        plt.title('æŸå¤±æ›²çº¿')
        plt.grid(True, alpha=0.3)
        plt.legend()

        # 4. æ¢ç´¢ç‡æ›²çº¿
        plt.subplot(3, 3, 4)
        plt.plot(self.stats['epsilon'], 'g-')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('æ¢ç´¢ç‡ (Îµ)')
        plt.title('æ¢ç´¢ç‡è¡°å‡')
        plt.grid(True, alpha=0.3)

        # 5. æ­¥æ•°æ›²çº¿
        plt.subplot(3, 3, 5)
        if len(self.stats['steps']) > window:
            steps_smooth = np.convolve(self.stats['steps'], np.ones(window) / window, mode='valid')
            plt.plot(range(window - 1, len(self.stats['steps'])), steps_smooth, 'c-', alpha=0.7, label='å¹³æ»‘')
        plt.plot(self.stats['steps'], 'c-', alpha=0.3, label='åŸå§‹')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('æ­¥æ•°')
        plt.title('æ¯å±€å¹³å‡æ­¥æ•°')
        plt.grid(True, alpha=0.3)
        plt.legend()

        # 6. è®°å¿†åº“å¤§å°
        plt.subplot(3, 3, 6)
        plt.plot(self.stats['memory_size'], 'y-')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('è®°å¿†æ•°é‡')
        plt.title('ç»éªŒå›æ”¾è®°å¿†åº“å¤§å°')
        plt.grid(True, alpha=0.3)

        # 7. å¯¹æ‰‹ç±»å‹åˆ†å¸ƒ
        plt.subplot(3, 3, 7)
        if self.stats['opponent_types']:
            unique_types, counts = np.unique(self.stats['opponent_types'], return_counts=True)
            plt.pie(counts, labels=unique_types, autopct='%1.1f%%', startangle=90)
            plt.title('å¯¹æ‰‹ç±»å‹åˆ†å¸ƒ')

        plt.suptitle(f"DQNè®­ç»ƒç»“æœ - {self.config['board_size']}x{self.config['board_size']} æ£‹ç›˜", fontsize=16)
        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        plot_path = os.path.join(self.config['log_dir'], "training_curves.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        self._log(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")


def train_dqn(opponent_type='random', episodes=1000, **kwargs):
    """
    è®­ç»ƒDQNçš„ä¾¿æ·å‡½æ•°

    å‚æ•°:
        opponent_type: å¯¹æ‰‹ç±»å‹ ('random', 'rule', 'mixed', 'self', 'previous')
        episodes: è®­ç»ƒè½®æ•°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
    """
    config = {
        'opponent_type': opponent_type,
        'total_episodes': episodes,
        **kwargs
    }

    trainer = DQNTrainer(config)
    return trainer.train()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒDQNäº”å­æ£‹AI')
    parser.add_argument('--opponent', type=str,
                        choices=['random', 'rule', 'mixed', 'self', 'previous'],
                        default='mixed', help='è®­ç»ƒå¯¹æ‰‹ç±»å‹')
    parser.add_argument('--episodes', type=int, default=2000, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--aggression', type=float, default=0.3,
                        help='è§„åˆ™AIçš„æ”»å‡»æ€§ï¼ˆ0-1ï¼‰')
    parser.add_argument('--mixed-ratio', type=float, default=0.5,
                        help='æ··åˆè®­ç»ƒä¸­ä½¿ç”¨è§„åˆ™AIçš„æ¦‚ç‡')
    parser.add_argument('--target-win-rate', type=float, default=0.7,
                        help='ç›®æ ‡èƒœç‡ï¼ˆè¾¾åˆ°åæå‰åœæ­¢ï¼‰')
    parser.add_argument('--previous-model', type=str, default='',
                        help='æ—§ç‰ˆæœ¬æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºpreviousæ¨¡å¼ï¼‰')
    parser.add_argument('--self-play-ratio', type=float, default=0.3,
                        help='è‡ªæˆ‘å¯¹å¼ˆæ¯”ä¾‹')

    args = parser.parse_args()

    print("ğŸš€ DQNè®­ç»ƒé…ç½®:")
    print(f"  å¯¹æ‰‹ç±»å‹: {args.opponent}")


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒDQNäº”å­æ£‹AI')
    parser.add_argument('--opponent', type=str,
                        choices=['random', 'rule', 'mixed', 'self', 'previous'],
                        default='mixed', help='è®­ç»ƒå¯¹æ‰‹ç±»å‹')
    parser.add_argument('--episodes', type=int, default=2000, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--aggression', type=float, default=0.3,
                        help='è§„åˆ™AIçš„æ”»å‡»æ€§ï¼ˆ0-1ï¼‰')
    parser.add_argument('--mixed-ratio', type=float, default=0.5,
                        help='æ··åˆè®­ç»ƒä¸­ä½¿ç”¨è§„åˆ™AIçš„æ¦‚ç‡')
    parser.add_argument('--target-win-rate', type=float, default=0.7,
                        help='ç›®æ ‡èƒœç‡ï¼ˆè¾¾åˆ°åæå‰åœæ­¢ï¼‰')
    parser.add_argument('--previous-model', type=str, default='',
                        help='æ—§ç‰ˆæœ¬æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºpreviousæ¨¡å¼ï¼‰')
    parser.add_argument('--self-play-ratio', type=float, default=0.3,
                        help='è‡ªæˆ‘å¯¹å¼ˆæ¯”ä¾‹')

    args = parser.parse_args()

    print("ğŸš€ DQNè®­ç»ƒé…ç½®:")
    print(f"  å¯¹æ‰‹ç±»å‹: {args.opponent}")
    print(f"  è®­ç»ƒè½®æ•°: {args.episodes}")
    print(f"  è§„åˆ™AIæ”»å‡»æ€§: {args.aggression}")
    print(f"  æ··åˆæ¯”ä¾‹: {args.mixed_ratio}")
    print(f"  ç›®æ ‡èƒœç‡: {args.target_win_rate}")
    print("=" * 50)

    try:
        # é…ç½®å‚æ•°
        config = {
            'opponent_type': args.opponent,
            'total_episodes': args.episodes,
            'rule_aggression': args.aggression,
            'mixed_ratio': args.mixed_ratio,
            'target_win_rate': args.target_win_rate,
            'previous_model_path': args.previous_model if args.previous_model else None,
            'self_play_ratio': args.self_play_ratio
        }

        # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
        trainer = DQNTrainer(config)
        agent, stats = trainer.train()

        # æœ€ç»ˆè¯„ä¼°
        print("\n" + "=" * 60)
        print("ğŸ“Š æœ€ç»ˆè¯„ä¼°")
        print("=" * 60)

        final_win_rate = trainer.evaluate_agent(num_games=50)
        print(f"ğŸ¯ æœ€ç»ˆèƒœç‡: {final_win_rate:.2%}")
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")

    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()

# ä¸éšæœºAIè®­ç»ƒï¼ˆæœ€ç®€å•ï¼Œé€‚åˆåˆå­¦è€…ï¼‰
#python train_dqn.py --opponent random --episodes 1000

# ä¸è§„åˆ™AIè®­ç»ƒï¼ˆä¸­ç­‰éš¾åº¦ï¼‰
#python train_dqn.py --opponent rule --episodes 2000 --aggression 0.5

# æ··åˆè®­ç»ƒï¼ˆæ¨èï¼Œæ•ˆæœæœ€å¥½ï¼‰
#python train_dqn_fixed.py --opponent mixed --episodes 2000 --mixed-ratio 0.5

# è‡ªæˆ‘å¯¹å¼ˆï¼ˆé«˜çº§ï¼Œéœ€è¦å·²æœ‰åŸºç¡€æ¨¡å‹ï¼‰
#python train_dqn_fixed.py --opponent self --episodes 3000

# ä¸æ—§ç‰ˆæœ¬è®­ç»ƒï¼ˆæŒç»­æ”¹è¿›ï¼‰
#python train_dqn_fixed.py --opponent previous --episodes 2000 --previous-model "saved_models/best_model.pth"